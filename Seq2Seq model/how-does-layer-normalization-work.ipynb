{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How does Layer Normalization Work?\n\nLayer normalization was initially intended to be used in Recurrent neural networks because the result of batch normalization is depending on the mini-batch size and it is not clear how to apply it to RNNs. But it actually became a thing after \"Attention is all you need\" and introduction of Transformer architecture.\nThe developers of Transformer architecture chose it as their preferred method of normalization throughout the model because it performs exceptionally well, especially in NLP tasks.\n\n**But what exactly is layer normalization, and why we should normalize our data? Let’s begin with the later question.**","metadata":{}},{"cell_type":"markdown","source":"## Benefits of Normalization\n\n<div align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https://i.ibb.co/0p0r6VW/Capture.png\">\n</div>\n\nNormalization is good for your model. It reduces training time, unbiases model to higher value features and doesn’t allow weights to explode all over the place and restricts them to a certain range. \nAll in all, It is undesirable to train a model with gradient descent with non-normalized features.\n\nThere are more then one way to perform normalization, two of which are presented here:\n\n<div align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https://i.ibb.co/BfgTjGQ/Picture1.png\">\n</div>\n\nthe main difference between these normalization methods is the way we calculate average and variance in order to normalize our data.\nYou are probably familiar with the one on the right, the **batch norm**. \n","metadata":{}},{"cell_type":"markdown","source":"## Batch Normalization\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https://i.ibb.co/60m5Dm3/Picture1.png\">\n</div>\n\nIn batch norm, we take all sentences in a batch, and for each feature in these sentences, we can find an average and a variance, which will be used to normalize the data in that feature. \n\nFor example, Imagine that we have a batch of 2 senteces: “`Popcorn popped.`” and “`Tea steeped.`” you can see that each sentence is displayed by a matrix in which each row represents a word:\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https://i.ibb.co/4YSxCqr/Picture1.png\">\n</div>\n\nIn batch norm, we take one feature and calculate the average and variance of it. \n\n<div align=\"center\">\n  <img width=\"320\" src=\"https://i.ibb.co/TPr8bM2/Picture1.png\">\n</div>\n\nAnd then normalize the data so that the average is near zero and variance is about one. Here is the formula:\n\n$$x_{norm}=\\frac{x-avg(x)}{\\sqrt{var(x)}}$$\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https://i.ibb.co/gDKgrLk/Picture1.png\">\n</div>\n\nOf course, we should repeat this for other features as well.","metadata":{}},{"cell_type":"markdown","source":"## Layer Normalization\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https://i.ibb.co/tpDnBBg/Picture1.png\">\n</div>\n\nIn the layer norm, we take the average and variance from all of the features of a single sentence. \nLet’s see what it means using the same two sentences: \n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https://i.ibb.co/4YSxCqr/Picture1.png\">\n</div>\n\nHere we don’t care about the fact that these two sentences are from the same batch. In order to obtain the average and variance, we simply use all of the features in every sentence:\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https://i.ibb.co/2ytx9Jh/Picture1.png\">\n</div>\n\nAnd again, after normalization, we’ll have matrices with average of 0 and variance of 1:\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https://i.ibb.co/42b38Q8/Picture1.png\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Layer Norm in code\n\nNow we want to implement what I just described in code. I create two numpy arrays, `sentence1` and `sentence2`, which are the same dummy matrices that we used in the illustrations:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nsentence1 = np.array([[0.31,0.14,0.93],[0.14,0.88,0.98]]) # \"Popcorn Popped.\"\nsentence2 = np.array([[0.85,0.2,0.14],[0.46,0.61,0.49]]) # \"Tea Steeped.\"","metadata":{"execution":{"iopub.status.busy":"2025-03-14T21:22:53.817735Z","iopub.execute_input":"2025-03-14T21:22:53.818129Z","iopub.status.idle":"2025-03-14T21:22:53.827660Z","shell.execute_reply.started":"2025-03-14T21:22:53.818040Z","shell.execute_reply":"2025-03-14T21:22:53.826799Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Now we Calculte Average and Variance for each of these Sentences. ","metadata":{}},{"cell_type":"code","source":"# Average and Variace for First Sentence:\naverage1 = sentence1.mean()\nvariance1 = sentence1.var()\n\n# Average and Variance for Second Sentence:\naverage2 = sentence2.mean()\nvariance2 = sentence2.var()","metadata":{"execution":{"iopub.status.busy":"2025-03-14T21:22:53.828599Z","iopub.execute_input":"2025-03-14T21:22:53.828915Z","iopub.status.idle":"2025-03-14T21:22:53.845262Z","shell.execute_reply.started":"2025-03-14T21:22:53.828869Z","shell.execute_reply":"2025-03-14T21:22:53.844415Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Now we normalize by applying following equation on our matrices:\n$$x_{norm}=\\frac{x-avg(x)}{\\sqrt{var(x)}}$$","metadata":{}},{"cell_type":"code","source":"# Sentence 1 Normalization:\nsentence1_norm = (sentence1 - average1)/(np.sqrt(variance1))\nprint(f\"Sentence1:\\n{sentence1}\\n\\n Sentence1 (Normalized):\\n{sentence1_norm}\\n\")\n\n# Sentence 2 Normalization:\nsentence2_norm = (sentence2 - average2)/(np.sqrt(variance2))\nprint(f\"Sentence2:\\n{sentence2}\\n\\n Sentence2 (Normalized):\\n{sentence2_norm}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-14T21:22:53.846663Z","iopub.execute_input":"2025-03-14T21:22:53.847053Z","iopub.status.idle":"2025-03-14T21:22:53.856800Z","shell.execute_reply.started":"2025-03-14T21:22:53.847014Z","shell.execute_reply":"2025-03-14T21:22:53.855721Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sentence1:\n[[0.31 0.14 0.93]\n [0.14 0.88 0.98]]\n\n Sentence1 (Normalized):\n[[-0.68074565 -1.1375618   0.98528975]\n [-1.1375618   0.85093206  1.11964744]]\n\nSentence2:\n[[0.85 0.2  0.14]\n [0.46 0.61 0.49]]\n\n Sentence2 (Normalized):\n[[ 1.63221997 -1.07657062 -1.32661282]\n [ 0.00694562  0.63205114  0.13196672]]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"You could also Simply use the [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) Implementation from Pytorch Library:","metadata":{}},{"cell_type":"code","source":"import torch\n\ntorch1 = torch.from_numpy(sentence1) # \"Popcorn Popped.\"\ntorch2 = torch.from_numpy(sentence2) # \"Tea Steeped.\"\n\nlayer_norm = torch.nn.LayerNorm(torch1.size())\n\n# Sentence 1 Normalization:\ntorch1_norm = layer_norm(torch1.float())\nprint(f\"Sentence1:\\n{torch1}\\n\\n Sentence1 (Normalized):\\n{torch1_norm}\\n\")\n\n# Sentence 2 Normalization:\ntorch2_norm = layer_norm(torch2.float())\nprint(f\"Sentence2:\\n{torch2}\\n\\n Sentence2 (Normalized):\\n{torch2_norm}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-14T21:22:53.857942Z","iopub.execute_input":"2025-03-14T21:22:53.858265Z","iopub.status.idle":"2025-03-14T21:22:55.622562Z","shell.execute_reply.started":"2025-03-14T21:22:53.858231Z","shell.execute_reply":"2025-03-14T21:22:55.621710Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sentence1:\ntensor([[0.3100, 0.1400, 0.9300],\n        [0.1400, 0.8800, 0.9800]], dtype=torch.float64)\n\n Sentence1 (Normalized):\ntensor([[-0.6807, -1.1375,  0.9853],\n        [-1.1375,  0.8509,  1.1196]], grad_fn=<NativeLayerNormBackward>)\n\nSentence2:\ntensor([[0.8500, 0.2000, 0.1400],\n        [0.4600, 0.6100, 0.4900]], dtype=torch.float64)\n\n Sentence2 (Normalized):\ntensor([[ 1.6321, -1.0765, -1.3265],\n        [ 0.0069,  0.6320,  0.1320]], grad_fn=<NativeLayerNormBackward>)\n","output_type":"stream"}],"execution_count":4}]}